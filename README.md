# ğŸš´ Sujet de travaux pratiques "Introduction Ã  la data ingÃ©nierie 
Ce projet consiste Ã  construire un pipeline pour la collecte, la transformation et l'analyse des donnÃ©es des systÃ¨mes de vÃ©los en libre-service de plusieurs villes franÃ§aises : Paris, Nantes, Toulouse et Strasbourg.  
L'objectif est de consolider et aggrÃ©ger ces donnÃ©es dans une base DuckDB pour permettre des analyses ultÃ©rieurs.

---

## ğŸ“¥ **Sources des DonnÃ©es**

- [API Paris](https://opendata.paris.fr/explore/dataset/velib-disponibilite-en-temps-reel/api/)  
- [API Nantes](https://data.nantesmetropole.fr/explore/dataset/244400404_stations-velos-libre-service-nantes-metropole-disponibilites/api/)  
- [API Toulouse](https://data.toulouse-metropole.fr/explore/dataset/api-velo-toulouse-temps-reel/api/)  
- [API Strasbourg](https://data.strasbourg.eu/explore/dataset/stations-velhop/api/)  
- [API Open Data Communes](https://geo.api.gouv.fr/communes)  

---

## ğŸ—‚ï¸ **Structure du Projet**

```plaintext
â”œâ”€â”€ dags/                     # DÃ©finitions des DAGs Airflow
â”‚   â””â”€â”€ pipeline.py           # Orchestration du pipeline
â”œâ”€â”€ data/                     # DonnÃ©es utilisÃ©es par les processus
â”‚   â”œâ”€â”€ duckdb/               # Base de donnÃ©es locale DuckDB
â”‚   â”œâ”€â”€ raw_data/             # DonnÃ©es brutes classÃ©es par date
â”‚   â””â”€â”€ sql_statements/       # RequÃªtes SQL rÃ©utilisables
â”œâ”€â”€ src/                      # Code source principal
â”‚   â”œâ”€â”€ __init__.py           # Fichier d'initialisation du module
â”‚   â”œâ”€â”€ data_aggregation.py   # AgrÃ©gation des donnÃ©es
â”‚   â”œâ”€â”€ data_consolidation.py # Consolidation des donnÃ©es brutes
â”‚   â”œâ”€â”€ data_ingestion.py     # Ingestion des donnÃ©es en temps rÃ©el
â”‚   â”œâ”€â”€ main.py               # Point d'entrÃ©e principal
â”‚   â””â”€â”€ query_duckdb.py       # RequÃªtes analytiques DuckDB
â”œâ”€â”€ images/                   # Images pour documentation et visualisation
â”œâ”€â”€ Dockerfile                # Configuration Docker pour Airflow
â”œâ”€â”€ docker-compose.yml        # Orchestration Docker Compose
â”œâ”€â”€ docker_requirements.txt   # DÃ©pendances Python spÃ©cifiques Ã  Docker
â”œâ”€â”€ requirements.txt          # Liste des dÃ©pendances Python
â””â”€â”€ README.md                 # Documentation du projet
```

---
### **RÃ©sumÃ© du Workflow du Projet**

### **1. Ingestion des donnÃ©es**
**Objectif** : RÃ©cupÃ©rer des donnÃ©es brutes depuis des sources externes.
#### Ã‰tapes : 
Dans le fichier python `data_ingestion.py`
- **`get_realtime_bicycle_data`** : 
  - RÃ©cupÃ¨re les donnÃ©es en temps rÃ©el sur les vÃ©los disponibles des villes (Paris, Nantes, Toulouse, Strasbourg).
- **`get_commune_data`** : 
  - RÃ©cupÃ¨re des donnÃ©es sur les communes.

#### Produits :
- Les donnÃ©es brutes sont enregistrÃ©es dans les fichiers JSON dans le rÃ©pertoire dÃ©diÃ©.

---

### **2. Consolidation des donnÃ©es**
**Objectif** : Organiser et structurer les donnÃ©es brutes pour prÃ©parer leur utilisation.

#### Ã‰tapes :
Dans le fichier python `data_consolidation.py`
- **`create_consolidate_tables`** :
  - CrÃ©e les tables nÃ©cessaires pour stocker les donnÃ©es consolidÃ©es.
- **`consolidate_city_data`** :
  - Structure et nettoie les donnÃ©es des communes pour les prÃ©parer Ã  l'analyse.
- **`consolidate_station_data`** :
  - PrÃ©pare et organise les informations sur les stations de vÃ©los.
- **`consolidate_station_statement_data`** :
  - Traite et structure les donnÃ©es liÃ©es aux Ã©tats des stations, comme le nombre de vÃ©los disponibles.

#### Produits :
- Les donnÃ©es consolidÃ©es sont enregistrÃ©es dans Duckdb et prÃªtes Ã  Ãªtre utilisÃ©es dans des Ã©tapes analytiques ou agrÃ©gÃ©es.

---

### **3. AgrÃ©gation des donnÃ©es**
**Objectif** : SynthÃ©tiser les donnÃ©es consolidÃ©es pour crÃ©er des vues ou mÃ©triques prÃªtes Ã  l'analyse.

#### Ã‰tapes :
Dans le fichier python `data_agregation.py`
- **`create_agregate_tables`** :
  - CrÃ©e les tables nÃ©cessaires pour stocker les donnÃ©es agrÃ©gÃ©es.
- **`agregate_dim_city`** :
  - Met Ã  jour la table dimensionnelle des villes (**DIM_CITY**) avec les donnÃ©es les plus rÃ©centes, telles que le nombre dâ€™habitants.
- **`agregate_dim_station`** :
  - Met Ã  jour la table dimensionnelle des stations (**DIM_STATION**) avec les informations consolidÃ©es les plus rÃ©centes.
- **`agregate_fact_station_statements`** :
  - Met Ã  jour la table factuelle des Ã©tats des stations (**FACT_STATION_STATEMENT**) en associant les informations des stations et des villes.

#### Produits :
- Les donnÃ©es finales sont stockÃ©es sous forme de tables agrÃ©gÃ©es dans Duckdb, prÃªtes pour des analyses ou des visualisations.

---

## ğŸš€ **Installation et ExÃ©cution**

### **Sans Orchestration Airflow**

1. **Cloner le dÃ©pÃ´t :**  
   ```bash
   git clone https://github.com/BrightMehou/ETL_VELO.git
   cd ETL_VELO
   ```

2. **CrÃ©er et activer un environnement virtuel :**  
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # Sous Windows : .venv\Scripts\activate
   ```

3. **Installer les dÃ©pendances :**  
   ```bash
   pip install -r requirements.txt
   ```

4. **ExÃ©cuter le script principal :**  
   ```bash
   python src/main.py
   ```

---

### **Avec Orchestration Airflow**

1. **Construire les images Docker :**  
   ```bash
   docker-compose build
   ```

2. **Initialiser la base de donnÃ©es Airflow :**  
   ```bash
   docker-compose run airflow-webserver airflow db init
   ```

3. **CrÃ©er un utilisateur administrateur pour Airflow :**  
   ```bash
   docker-compose run airflow-webserver airflow users create \
       --username admin \
       --password admin \
       --firstname Admin \
       --lastname User \
       --role Admin \
       --email admin@example.com
   ```

4. **Lancer les services Airflow :**  
   ```bash
   docker-compose up -d
   ```

5. **AccÃ©der Ã  l'interface Airflow :**  
   Rendez-vous sur [http://localhost:8080](http://localhost:8080) et connectez-vous avec le nom d'utilisateur `admin` et le mot de passe `admin`.

---

## ğŸ“Š **Analyse des donnÃ©es**

ExÃ©cutez le script pour interroger les donnÃ©es consolidÃ©es :  
```bash
python src/query_duckdb.py
```

vous devez obtenir les rÃ©sultats des requÃªtes suivantes

#### 1. Nombre d'emplacements disponibles pour les vÃ©los dans une ville :
```sql
SELECT dm.NAME, tmp.SUM_BICYCLE_DOCKS_AVAILABLE
FROM DIM_CITY dm
INNER JOIN (
    SELECT CITY_ID, SUM(BICYCLE_DOCKS_AVAILABLE) AS SUM_BICYCLE_DOCKS_AVAILABLE
    FROM FACT_STATION_STATEMENT
    WHERE CREATED_DATE = (SELECT MAX(CREATED_DATE) FROM CONSOLIDATE_STATION)
    GROUP BY CITY_ID
) tmp ON dm.ID = tmp.CITY_ID
WHERE lower(dm.NAME) IN ('paris', 'nantes', 'strasbourg', 'toulouse');
```

#### 2. Moyenne des vÃ©los disponibles par station :
```sql
SELECT ds.NAME, ds.CODE, ds.ADDRESS, tmp.AVG_DOCK_AVAILABLE
FROM DIM_STATION ds
JOIN (
    SELECT STATION_ID, AVG(BICYCLE_AVAILABLE) AS AVG_DOCK_AVAILABLE
    FROM FACT_STATION_STATEMENT
    GROUP BY STATION_ID
) tmp ON ds.ID = tmp.STATION_ID;
```

Vous pouvez Ã©galement modifirer le fichier src/query_duckdb.py pour exÃ©cuter vos propres requÃªtes ou tÃ©lÃ©charger l'exÃ©cutable suivant sur le site de Duckdb.

[Duckdb installation](https://duckdb.org/docs/installation/)
